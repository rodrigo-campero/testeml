{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr13J8Thvm_t"
   },
   "source": [
    "# Ensemble : algoritmo Gradient Boosting\n",
    "\n",
    "O objetivo é avaliar se um nódulo de mama é maligno ou benigno. Trata-se de dados de classificação de câncer de mama.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de Machine Learning para detecção do câncer de mama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is downloaded from: https://goo.gl/U2Uwz2\n",
    "\n",
    "Os dados também estão disponibilizados pela própria biblioteca, que possui um pacote de Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "c) perimeter\n",
    "d) area\n",
    "e) smoothness (local variation in radius lengths)\n",
    "f) compactness (perimeter^2 / area - 1.0)\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "h) concave points (number of concave portions of the contour)\n",
    "i) symmetry\n",
    "j) fractal dimension (\"coastline approximation\" - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo Gradient Boosting em um modelo de classificação.\n",
    "\n",
    "De acordo com a documentação do Scikit-Learn, os parâmetros mais importantes na concepção do modelo são:\n",
    "learning_rate: Taxa de aprendizado que determina a importância de cada árvore na concepção do modelo final e na minimização do resíduo gerado. Deve ser um valor no intervalo de 0.0 a 1.\n",
    "n_estimators: Número de árvores ou estágios utilizados na construção e treinamento do modelo.\n",
    "Também é possível controlar o tamanho de cada árvore através dos parâmetros:\n",
    "max_depth: A profundidade máxima da árvore.\n",
    "max_leaf_nodes: Número máximo de folhas.\n",
    "\n",
    "Existem outros parâmetros que podem ser observados na documentação oficial.\n",
    "\n",
    "Vamos criar um modelo simples com alguns desses parâmetros definidos de forma default, pelo próprio algoritmo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar o dataset em csv\n",
    "df = pd.read_csv(breast-cancer-wisconsin.csv)\n",
    "# eliminar uma coluna com erro\n",
    "df.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com exceção da coluna diagnosis, todas as outras são do tipo numérica (int e float). Apesar dessa ser a nossa conclusão olhando as primeiras entradas, é prudente analisar por meio do atributo dtypes para ter certeza que nenhuma foi importada como string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensões do df\n",
    "print(\"DIMENSÕES DO DATAFRAME:\")\n",
    "print(\"Linhas:\\t\\t{}\".format(df.shape[0]))\n",
    "print(\"Colunas:\\t{}\".format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a variável target\n",
    "\n",
    "#opção 1\n",
    "df['target'] = df['diagnosis'].map({'B': 0, 'M': 1}).astype(int)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(['id','diagnosis'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\n",
    "\n",
    "plt.title('Correlation between different fearures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "sns.boxplot(x='target', y='perimeter_worst', data=df)\n",
    "plt.title('perimeter_worst x diagnosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise exploratória dos dados usando o gráfico Box Plot\n",
    "features = df\n",
    "fig,axs=plt.subplots(nrows=6, ncols=5, figsize=(18,10))\n",
    "for col, ax in zip(features[0:], axs.ravel()):\n",
    "    x=df.loc[:, col]\n",
    "    sns.boxplot(x, ax=ax, orient='v')\n",
    "    plt.subplots_adjust(top=0.92,bottom=0.08, left=0.10,right=0.95,hspace=0.25,wspace=0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise exploratória dos dados usando o gráfico histograma\n",
    "\n",
    "features = df\n",
    "fig,axs=plt.subplots(nrows=5, ncols=6, figsize=(18,10))\n",
    "for col, ax in zip(features[0:], axs.ravel()):\n",
    "    x=df.loc[:, col]\n",
    "    sns.distplot(x, ax=ax, color=\"blue\", kde=False)\n",
    "    plt.subplots_adjust(top=0.92,bottom=0.08, left=0.10,right=0.95,hspace=0.25,wspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# separar as variáveis independentes da variável alvo\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1),df['target'],test_size=0.30, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentação sobre XGBoost https://xgboost.readthedocs.io/en/stable/index.html\n",
    "    \n",
    "class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predito = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_predito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acurácia %0f' % model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos analisar o comportamento do algoritmo em cada etapa de ajuste através de um método chamado staged_predict.\n",
    "Uma característica interessante desse método é que ele possibilita que façamos análises de desempenho que poderiam ser realizadas somente no modelo final, como medir a acurácia ou o erro médio do modelo. \n",
    "Essas análises são importantes, pois, pode nos auxiliar a determinar melhores parâmetros para o modelo.\n",
    "A taxa de aprendizado determina a importância de cada árvore na concepção do modelo final e na minimização do resíduo gerado. Agora, utilizando o método staged_predict, vamos analisar o comportamento do algoritmo com alguns valores de learning_rate e analisar qual estágio o algoritmo irá atingir a minimização máxima do resíduo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo a documentação do algoritmo, existe uma troca entre os parâmetros n_estimators e learning_rate é evidente pelo gráfico gerado pelo algoritmo, que, em modelos treinados com uma taxa de aprendizado muito pequena, são necessários mais estágios para se chegar ao resíduo mínimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É recomendado a utilização de valores pequenos para o parâmetro learning_rate, visando manter a estabilidade do erro. \n",
    "Os valores recomendados são menores ou igual a 0.1, valor default definido pela biblioteca.\n",
    "Se no exemplo anterior conseguimos identificar em qual estágio obtivemos o menor resíduo para cada taxa de aprendizado, então também podemos utilizar esse método para obter o número de ajustes ideal para o nosso modelo. \n",
    "Treinamos nosso modelo com 200 árvores com a taxa de 0.1 e limitamos a profundidade da árvore a 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(max_depth=2, n_estimators=200, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)\n",
    "plt.ylabel('Resíduo')\n",
    "plt.xlabel('Número de estágios/árvores')\n",
    "plt.show()\n",
    "print('Acurácia %0f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_estimators = np.argmin(errors) + 1\n",
    "best_model = GradientBoostingClassifier(max_depth=2, n_estimators=best_n_estimators)\n",
    "best_model.fit(X_train, y_train)\n",
    "errors_best_model = [mean_squared_error(y_test, y_pred) for y_pred in best_model.staged_predict(X_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors_best_model)\n",
    "plt.ylabel('Resíduo')\n",
    "plt.xlabel('Número de estágios/árvores')\n",
    "plt.show()\n",
    "print('Acurácia %0f' % best_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos de apenas seis estágios de ajustes para melhorar nosso modelo, a acurácia agora foi de 96.27%, uma diferença pequena mas que pode ser um diferencial em problemas mais robustos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O número de estágios que definimos no início funcionará como um número máximo de estágios e o nosso algoritmo sempre decidirá qual é o número ideal de estágios para uma melhor performance do nosso modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvores = []\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    # Declara o modelo\n",
    "    model = GradientBoostingClassifier(max_depth=2, n_estimators=1000, learning_rate=learning_rate)\n",
    "    # Treina o modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    # Obtem os erros/residuos encontrados em cada estagio do algoritmo\n",
    "    errors = [mean_squared_error(y_test, y_pred) for y_pred in model.staged_predict(X_test)]\n",
    "    best_number_of_estimators = np.argmin(errors)\n",
    "    arvores.append(best_number_of_estimators + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rates, arvores)\n",
    "plt.ylabel('Numero de arvores/estágios')\n",
    "plt.xlabel('Taxa de aprendizado')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acurácia %0f' % best_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Matriz de Confusão\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Resultado do classification_report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Boosting\n",
    "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing LightGBM (Required in Jupyter Notebook and \n",
    "# few other compilers once)\n",
    "\n",
    "!pip install lightgbm\n",
    "  \n",
    "# Importing Required Library\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "  \n",
    "# Similarly LGBMRegressor can also be imported for a regression model.\n",
    "from lightgbm import LGBMClassifier\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Creating an object for model and fitting it on training data set \n",
    "model = LGBMClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM classifier.\n",
    "\n",
    "__init__(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns parâmetros importantes::\n",
    "\n",
    "max_depth: define um limite para a profundidade da árvore. O valor padrão é 20. É eficaz no controle do ajuste.\n",
    "categorical_feature: especifica o recurso categórico usado para o modelo de treinamento. \n",
    "bagging_fraction: especifica a fração de dados a ser considerada para cada iteração.\n",
    "num_iterations: especifica o número de iterações a serem realizadas. O valor padrão é 100.\n",
    "num_leaves: especifica o número de folhas em uma árvore. Deve ser menor que o quadrado de max_depth .\n",
    "max_bin: especifica o número máximo de caixas para armazenar os valores do recurso.\n",
    "min_data_in_bin: especifica a quantidade mínima de dados em um bin.\n",
    "feature_fraction : especifica a fração de recursos a serem considerados em cada iteração. O valor padrão é um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Predicting the Target variable\n",
    "pred = model.predict(X_test)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste de parâmetro\n",
    "Alguns parâmetros importantes e seu uso estão listados abaixo:\n",
    "\n",
    "max_depth: define um limite para a profundidade da árvore. O valor padrão é 20. É eficaz no controle do ajuste.\n",
    "categorical_feature: especifica o recurso categórico usado para o modelo de treinamento. \n",
    "bagging_fraction: especifica a fração de dados a ser considerada para cada iteração.\n",
    "num_iterations: especifica o número de iterações a serem realizadas. O valor padrão é 100.\n",
    "num_leaves: especifica o número de folhas em uma árvore. Deve ser menor que o quadrado de max_depth .\n",
    "max_bin: especifica o número máximo de caixas para armazenar os valores do recurso.\n",
    "min_data_in_bin: especifica a quantidade mínima de dados em um bin.\n",
    "tarefa: especifica a tarefa que desejamos realizar, que é treinamento ou previsão. A entrada padrão é train . Outro valor possível para este parâmetro é a previsão.\n",
    "feature_fraction : especifica a fração de recursos a serem considerados em cada iteração. O valor padrão é um."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentação sobre XGBoost https://xgboost.readthedocs.io/en/stable/index.html\n",
    "\n",
    "!pip install  xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# Import XGBoost\n",
    "import xgboost\n",
    "# XGBoost Classifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=500, max_depth=5, learning_rate=0.01, n_jobs=-1)\n",
    "bst = xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predito = xgb.predict(X_test)\n",
    "accuracy_score(y_test, y_predito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Matriz de Confusão\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_predito)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=['0', '1'])\n",
    "disp.plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predito))\n",
    "\n",
    "# Resultado do classification_report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "    \n",
    "# Plot feature importance\n",
    "plot_importance(xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = xgb.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(40, columns=\"score\").plot(kind='barh', figsize = (20,10)) ## plot top 40 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separar as variáveis independentes da variável alvo\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "# padronizar as colunas numéricas\n",
    "X= StandardScaler().fit_transform(X)\n",
    "# label encoder na variável alvo\n",
    "y= LabelEncoder().fit_transform(y)\n",
    "\n",
    "# dividir o dataset entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# instanciando o modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators = 10,max_depth=3, criterion='entropy', random_state = 17)\n",
    "# treinando o modelo\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# realizar as previsões no dataset de teste\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# ver acurácia geral\n",
    "print('[Acurácia] Random Forest:', accuracy_score(y_test, y_pred))\n",
    "# imprimir o classification report\n",
    "print('\\n[Classification Report] Random Forest')\n",
    "print( classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar a matriz de confusão\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "             index=['neg', 'pos'], columns=['pred_neg', 'pred_pos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Classificação usando Ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
